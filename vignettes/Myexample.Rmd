---
title: "Myexample"
author: "Wang Kexin"
date: "2023-11-27"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Myexample}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

## Example 1

Consider a problem: the observed data come from M candidate linear models
$$
y_i \mid\left(\rho_m, \beta_m, \sigma_m^2, m=1, \ldots, M\right) \sim \sum_{m=1}^M \rho_m \cdot N\left(x_i^{\prime} \beta_m, \sigma_m^2\right),
$$
where $\rho=(\rho_1,..,\rho_M)$ is the proportion vector of the M sub-populations, with$\rho_m\ge0$and$\sum_{m=1}^M\rho_M=1$.$\beta_m=(\beta_{m1},...,\beta_{mp})'$is the coefficient vector for the linear regression in the mth sub-population.$\sigma_m^2$is the corresponding variance of the Gaussian residual errors.

we introduce a latent variable $z_i$ for each observation i, so that $z_i = m$ indicates that the ith observation comes from the mth sub-population.
$$z_i\sim Multinomial(\rho_1,...\rho_M),\ \ \ y_i|z_i\sim N(x_i'\beta_m,\sigma_m^2)$$
Then we use the gibbs sampling to update $\rho$,z,and $\beta$ to gain the true model and see which model these data comes from.

Using 3 candidate models and using the following code to generate the initial model.

```{r}
library(SA23204174)
library(MASS)
library(corrplot)
library(gridExtra)
library(stargazer)
library(MCMCpack)
library(matrixcalc)
library(MASS)
n_obs = 200
n_coeff = 5
rho1 = 0.2
rho2 = 0.3
rho3 = 0.5

true_proportions = c(rho1, rho2, rho3)*n_obs

intercept_beta = 0.5
beta1 = c(1,-2,3,2,4)
beta2 = c(-1,1,1,-2,-2)
beta3 = c(-1,-2,-3,2,-1)

# build covariance matrix
covariance_matrix = matrix(0.5, nrow = length(beta1), ncol = (length(beta1)))
for(i in 1:length(beta1))
{
  for(j in 1:length(beta1))
  {
    covariance_matrix[i,j] = covariance_matrix[i,j]^abs(i - j)
  }
}

dataset = data.matrix(data.frame(mvrnorm(n = n_obs,mu = rep(0,length(beta1)),Sigma = covariance_matrix)))
dataset = cbind(1, dataset)
colnames(dataset)[1] = 'intercept'

beta1 = c(intercept_beta,beta1)
beta2 = c(intercept_beta,beta2)
beta3 = c(intercept_beta,beta3)

y = c()
# beta1
for(i in 1:true_proportions[1])
{
  new_y = dataset[i,]%*%beta1
  y = c(y, new_y)
}

# beta2
for(i in (true_proportions[1]+1): (true_proportions[1]+true_proportions[2]))
{
  new_y = dataset[i,]%*%beta2
  y = c(y, new_y)
}

# beta3
for(i in (1 + true_proportions[1]+true_proportions[2]):n_obs)
{
  new_y = dataset[i,]%*%beta3
  y = c(y, new_y)
}

# add error
for(i in 1:length(y))
{
  y[i] = rnorm(1,0,1) + y[i]
}

TIMES = 1000

M = 3
X = as.matrix(dataset)
X = as.matrix(X[,order(colnames(X))])
```

Then let's initialize every parameter.

```{r}
initialized = init(X, y, M)

theta_array= initialized$theta
#beta_test1 = as.matrix(beta1)
#row.names(beta_test1) = row.names(theta_array$beta[[1]])

#beta_test2 = as.matrix(beta2)
#row.names(beta_test2) = row.names(theta_array$beta[[2]])

#theta_array$beta[[1]] =  beta_test1
#theta_array$beta[[2]] = beta_test2

z_array = initialized$matrix_z
history_theta = list()
history_z  = list()

```

Using the gibbs sampling to update every parameter.

```{r}
for(i in 1:TIMES)
{
  new_z = UpdateArrayZ(theta_array,y,X,M)
  history_z[[i]] = new_z
  z_array=new_z
  #print('rho')
  theta_array = UpdateRho(theta_array,z_array,M)
  #print('subpop')
  covariates = UpdateSubPop(theta_array,z_array, X,M)
  #print('W_m inv')
  theta_array = UpdateW_inv(theta_array, covariates, M)
  #print('sigma')
  #print(theta_array$sigma)
  theta_array = UpdateSigma(theta_array, z_array,covariates,M,y)
  #print('beta')
  theta_array = UpdateBeta(z_array,covariates,theta_array,M,y)
  #print(theta_array$sigma)
  #print('r')
  #theta_array = UpdateR(theta_array, z_array, X,y)
  #print(theta_array$r)
  history_theta[[i]] = theta_array
}
```

```{r}
n_covariates = 5
summary_parameters = data.frame(matrix(ncol = n_covariates+1, nrow = M))
colnames(summary_parameters) = row.names(theta_array$old_beta[[1]])

beta_estimates = list()
for(idx_m in 1:M){
  for(theta in history_theta){
    beta_estimates[[toString(idx_m)]] = rbind(beta_estimates[[toString(idx_m)]], theta$beta[[idx_m]])
  }
}
```

Plot the estimated data to compare the result.

```{r}
for(idx_m in 1:length(beta_estimates))
{
  print('Subpopulation n:')
  print(idx_m)
  par(mfrow=c(2,3))
  list_covariates = sort(unique(row.names(beta_estimates[[idx_m]])))
  for(one_cov in list_covariates)
  {
    values = beta_estimates[[idx_m]][which(row.names(beta_estimates[[idx_m]])==one_cov),]
    one_median = median(values)
    summary_parameters[idx_m,one_cov] = one_median
    plot(values,type = 'l', main = one_cov)
  }
}
```

We can see that the estimated coefficients of each model is close to the true value.

## Example 2

Using the EM algorithm to solve the moth gene problem. If we know the number of different color of the moth,that Nc,Ni,Nt, but we just want to the propotion of the three gene$p_c,p_i,p_t$, we can use the EM algorithm to compute the true propotion $p_c,p_i,p_t$.

```{r}
library(SA23204174)
p=MyEM(156,185,95)
```

That's to say, the three propotion is $p_c=0.1879971,\ \ p_i=0.3033539,\ \ p_t=0.508649$.
