---
title: "Myexample"
author: "Wang Kexin"
date: "2023-11-27"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Myexample}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

## Example 1

Consider a problem: the observed data come from M candidate linear models
$$
y_i \mid\left(\rho_m, \beta_m, \sigma_m^2, m=1, \ldots, M\right) \sim \sum_{m=1}^M \rho_m \cdot N\left(x_i^{\prime} \beta_m, \sigma_m^2\right),
$$
where $\rho=(\rho_1,..,\rho_M)$ is the proportion vector of the M sub-populations, with$\rho_m\ge0$and$\sum_{m=1}^M\rho_M=1$.$\beta_m=(\beta_{m1},...,\beta_{mp})'$is the coefficient vector for the linear regression in the mth sub-population.$\sigma_m^2$is the corresponding variance of the Gaussian residual errors.

we introduce a latent variable $z_i$ for each observation i, so that $z_i = m$ indicates that the ith observation comes from the mth sub-population.
$$z_i\sim Multinomial(\rho_1,...\rho_M),\ \ \ y_i|z_i\sim N(x_i'\beta_m,\sigma_m^2)$$
Then we use the gibbs sampling to update $\rho$,z,and $\beta$ to gain the true model and see which model these data comes from.

Using 3 candidate models and using the following code to generate the initial model.

```{r}
library(SA23204174)
library(MASS)
library(corrplot)
library(gridExtra)
library(stargazer)
library(MCMCpack)
library(matrixcalc)
library(MASS)
n_obs = 200
n_coeff = 5
rho1 = 0.2
rho2 = 0.3
rho3 = 0.5

true_proportions = c(rho1, rho2, rho3)*n_obs

intercept_beta = 0.5
beta1 = c(1,-2,3,2,4)
beta2 = c(-1,1,1,-2,-2)
beta3 = c(-1,-2,-3,2,-1)

# build covariance matrix
covariance_matrix = matrix(0.5, nrow = length(beta1), ncol = (length(beta1)))
for(i in 1:length(beta1))
{
  for(j in 1:length(beta1))
  {
    covariance_matrix[i,j] = covariance_matrix[i,j]^abs(i - j)
  }
}

dataset = data.matrix(data.frame(mvrnorm(n = n_obs,mu = rep(0,length(beta1)),Sigma = covariance_matrix)))
dataset = cbind(1, dataset)
colnames(dataset)[1] = 'intercept'

beta1 = c(intercept_beta,beta1)
beta2 = c(intercept_beta,beta2)
beta3 = c(intercept_beta,beta3)

y = c()
# beta1
for(i in 1:true_proportions[1])
{
  new_y = dataset[i,]%*%beta1
  y = c(y, new_y)
}

# beta2
for(i in (true_proportions[1]+1): (true_proportions[1]+true_proportions[2]))
{
  new_y = dataset[i,]%*%beta2
  y = c(y, new_y)
}

# beta3
for(i in (1 + true_proportions[1]+true_proportions[2]):n_obs)
{
  new_y = dataset[i,]%*%beta3
  y = c(y, new_y)
}

# add error
for(i in 1:length(y))
{
  y[i] = rnorm(1,0,1) + y[i]
}

TIMES = 1000

M = 3
X = as.matrix(dataset)
X = as.matrix(X[,order(colnames(X))])
```

Then let's the init function to initialize every parameter. we define the vector $\rho_0$ of the mixture proportions:  it's built sampling the values from a conjugate Dirichlet prior distribution:
$$\rho \sim Dirichlet(\alpha_1,..., \alpha_M)$$
In each mixture component of the regression model, the prior distributions of the indicator variables $r_{mj}$ are assumed to be independent $Bernoulli(d_{mj})$  for $j = 1 , . . . , p$. So, looking at the joint distribution of $r_m$:
$$\pi(r_m) = \prod_{j=1}^p d_{mj} (1- d_{mj})^{1-r_{mj}}$$
We can sample the initial value of each $r_{mj}$ from a non-informative $Bernoulli(0.5)$.Each $z_i$ is generated from a multinomial distribution, i.e. $z_i \sim Multinomial(\rho_1,.., \rho_M)$.The prior of each $\beta_m(r_m)$ is assumed to be the following g-prior:
$$\beta_m(r_m) \sim N \bigg ( \hat \beta_m ^{\lambda_m} (r_m), g_m \sigma_m^2 [X_m'(r_m)X_m(r_m)]^{-1} \bigg )$$
where:

- $\hat \beta_m ^{\lambda_m} (r_m) = w_m(r_m) X_m'(r_m) Y_m$

- $g_m$ is a positive arbitrary number to fix. It's usually equal to $n_m$, which is the size of the sub-population m. 

- $w_m(r_m) = [X_m'(r_m) X_m(r_m) + \lambda_m I]^{-1}$

- $\lambda_m$ is the ridge parameter: it's used when we cannot derive the inverse of $X_m'(r_m) X_m(r_m)$. With $\lambda_m = 0$ we have $w_m(r_m) = [X_m'(r_m) X_m(r_m)]^{-1}$

- $\sigma_m^2 \sim IG \big ( \frac{a_{m_0}}{2}, \frac{b_{m_0}}{2} \big )$. We could assign to $a_{m_0}$ and $b_{m_0}$ some non-informative values (i.e. both equals to 0.01) or we could even fix it equals to 1, in order to avoid uncontrolled values sampled of $\sigma_m$.


```{r}
initialized = init(X, y, M)

theta_array= initialized$theta
#beta_test1 = as.matrix(beta1)
#row.names(beta_test1) = row.names(theta_array$beta[[1]])

#beta_test2 = as.matrix(beta2)
#row.names(beta_test2) = row.names(theta_array$beta[[2]])

#theta_array$beta[[1]] =  beta_test1
#theta_array$beta[[2]] = beta_test2

z_array = initialized$matrix_z
history_theta = list()
history_z  = list()

```

Using the gibbs sampling to update every parameter.

First, update latent variable Z**. We update the values of the latent variable z which determines the structure of each group. To do this, for each observation we sample the z value form the following conditional distribution:

$$P(z_i =  m |\theta, y) = \frac{\rho_m f(y_i|\theta_m)}{\sum_{m=1}^M \rho_m f(y_i|\theta_m)}$$ 
In other words, for each sub-population we compute the probability to assign a specific observation to it, then we sample the value of z from each distribution. The density function at the numerator corresponds to the normal density related to y, where:

$$[y_i| \theta_m] \sim N \bigg (x_i(r_m) \beta_m(r_m), \sigma_m^2 \bigg )$$
Where $\theta_m = (\beta_m, \sigma_m^2, \rho_m, r_m)$.

Then we update $\rho$.The conditional distribution of $\rho$, given z, is the following:

$$\rho \sim Dirichlet (n_1 + \alpha_1,..., n_M + \alpha_M)$$
Where $n_1, n_2,..., n_M$ are the cardinality of each sub-population.

Third, we update sub-populations.At each iteration we update the sub-populations, looking at the two latent variables z and r.

Then we update $w_m(r_m)$. As for the previous step, we need to update the matricies involved in our algorithm: in this case, we update the $p \times p$ matrix $w_m(r_m)$. At each iteration we check whether the updated matrix  $X_m'(r_m) X_m(r_m)$ is singular or not: in the first case, the ridge parameter $\lambda_m$ is added to the computed matrix otherwise not. This parameter is equal to $1/p$, where p is the number of covariates used in the specific sub-population. Adding $\lambda_m$ we have $w_m(r_m) = \big (X_m'(r_m) X_m(r_m) + \lambda_mI \big )^{-1}$

Then we update $\sigma_m$ and $\hat \beta_m(r_m)$.** In order to sample $\sigma_m^2$ we need first to update the parameter $\hat \beta_m^{\lambda_m}(r_m)$, where:
$$\hat \beta_m^{\lambda_m} (r_m) = w_m(r_m) X_m'(r_m)Y_m$$
So now, we need to sample the variance of each subpopulation, updating first the parameters of the full conditional, which is an inverse gamma:
$$\sigma_m^2 \sim IG \bigg (\frac{a_m}{2}, \frac{b_m}{2} \bigg )$$
We have:

- $a_m = n_m + q_m + a_{m_0}$, where $q_m = \sum_i^p r_{mj}$, i.e. the number of active covariates in the sub-population

- $b_m = \big [ Y_m - X_m(r_m) \beta_m(r_m) \big ]'\big [ Y_m - X_m(r_m) \beta_m(r_m) \big ] + \frac{\big [ \beta_m(r_m) - \beta_m^{\lambda_m}(r_m) \big ]' w_m^{-1}(r_m) \big [ \beta_m(r_m) - \beta_m^{\lambda_m}(r_m) \big ]}{ n_m} + b_{m_0}$

In this case, the sampled parameters depends on $z, r_m, \beta_m$

Finally, we update $\beta$.The conditional distribution of $\beta_m$ is given by a multivariate normal distribution (MVN). Indeed, we have:
$$\beta_m(r_m) \sim MVN \bigg (\mu_m, \Omega_m \bigg )$$
Where:

- $\mu_m = \Omega_m  \big ( \frac{ n_m X_m (r_m) Y_m' + w_m^(-1)(r_m) \hat \beta_m^{\lambda_m} (r_m)}{n_m \sigma_m^2}  \big )$ ($p \times 1$ vector) 

- $\Omega_m^{-1} = \big [ \frac{n_m X'_m (r_m) X_m (r_m) + w_m^{-1} (r_m)}{n_m \sigma_m^2}  \big ]$ ($p \times p$ matrix)


```{r}
for(i in 1:TIMES)
{
  new_z = UpdateArrayZ(theta_array,y,X,M)
  history_z[[i]] = new_z
  z_array=new_z
  #print('rho')
  theta_array = UpdateRho(theta_array,z_array,M)
  #print('subpop')
  covariates = UpdateSubPop(theta_array,z_array, X,M)
  #print('W_m inv')
  theta_array = UpdateW_inv(theta_array, covariates, M)
  #print('sigma')
  #print(theta_array$sigma)
  theta_array = UpdateSigma(theta_array, z_array,covariates,M,y)
  #print('beta')
  theta_array = UpdateBeta(z_array,covariates,theta_array,M,y)
  #print(theta_array$sigma)
  #print('r')
  #theta_array = UpdateR(theta_array, z_array, X,y)
  #print(theta_array$r)
  history_theta[[i]] = theta_array
}
```

```{r}
n_covariates = 5
summary_parameters = data.frame(matrix(ncol = n_covariates+1, nrow = M))
colnames(summary_parameters) = row.names(theta_array$old_beta[[1]])

beta_estimates = list()
for(idx_m in 1:M){
  for(theta in history_theta){
    beta_estimates[[toString(idx_m)]] = rbind(beta_estimates[[toString(idx_m)]], theta$beta[[idx_m]])
  }
}
```

Plot the estimated data to compare the result.

```{r}
for(idx_m in 1:length(beta_estimates))
{
  print('Subpopulation n:')
  print(idx_m)
  par(mfrow=c(2,3))
  list_covariates = sort(unique(row.names(beta_estimates[[idx_m]])))
  for(one_cov in list_covariates)
  {
    values = beta_estimates[[idx_m]][which(row.names(beta_estimates[[idx_m]])==one_cov),]
    one_median = median(values)
    summary_parameters[idx_m,one_cov] = one_median
    plot(values,type = 'l', main = one_cov)
  }
}
```

We can see that the estimated coefficients of each model is close to the true value.

## Example 2

Using the EM algorithm to solve the moth gene problem. If we know the number of different color of the moth,that Nc,Ni,Nt, but we just want to the propotion of the three gene$p_c,p_i,p_t$, we can use the EM algorithm to compute the true propotion $p_c,p_i,p_t$.

```{r}
library(SA23204174)
p=MyEM(156,185,95)
```

That's to say, the three proportion is $p_c=0.1879971,\ \ p_i=0.3033539,\ \ p_t=0.508649$.
